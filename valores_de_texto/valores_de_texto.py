# -*- coding: utf-8 -*-
"""valores_de_texto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ezUjPBUL6hqRnhJlE7dtIoH4bE5YTdUr

Nós acabamos de ver como podemos tratar valores numéricos faltantes. 

Quando fizemos isso, deixamos os valores que não eram numéricos, do tipo **object** fora do nosso modelo.

Quando temos valores que não são numéricos e descrevem algo, no nosso caso, são texto, eles são chamados de valores **categóricos**.

Agora, o nosso modelo entende texto? Não, ele entende apenas números, então, até agora estamos jogando toda a informação que podemos ter do texto fora. Será que tem alguma outra maneira de lidar com isso?

Tem sim, vamos novamente fazer os imports, carregar e limpar nossos dados.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor

"""Vou também já separar o nosso y, a coluna SalePrice do nosso X."""

dados_treino = pd.read_csv('treino.csv', index_col='Id')
dados_teste = pd.read_csv('teste.csv', index_col='Id')

"""Primeiro retiramos as linhas com os valores faltantes, se houver."""

X = dados_treino.copy()

X.dropna(axis=0, subset=['SalePrice'], inplace=True)

"""Criamos o y"""

y = X.SalePrice

"""Agora podemos excluir essa coluna do X."""

X.drop(['SalePrice'], axis=1, inplace=True)

"""Feito isso, vamos descobrir quantas colunas de X tem valores faltantes e dropa-las dos nossos dados."""

colunas_com_valores_faltantes = [coluna for coluna in X.columns if X[coluna].isnull().any()]

colunas_com_valores_faltantes

"""Podemos fazer isso porque são poucas.
Repare que temos 80 colunas no nossos dados de treino, ao tirarmos o y, que é sale price, ficaremos com 79.
"""

len(dados_treino.columns)

"""Dessas 79, 19 tem valores faltantes, então ficaremos com 79-19 colunas, 60. 
Essas 19 colunas representam 24% dos nossos dados.
"""

len(colunas_com_valores_faltantes)

X.drop(colunas_com_valores_faltantes, axis=1, inplace=True)

len(X.columns)

"""Precisamos fazer o mesmo para X teste para ficar justo."""

X_teste = dados_teste.copy()

X_teste.drop(colunas_com_valores_faltantes, axis=1, inplace=True)

len(X_teste.columns)

"""Tudo certo até aqui. Agora podemos dividir os nossos dados entre treino e validação."""

X_treino, X_valid, y_treino, y_valid = train_test_split(X, y,
                                                        train_size=0.8,
                                                        test_size=0.2,
                                                        random_state=42)

"""Vamos dar uma olhada no nosso X_treino."""

X_treino.head(5)

"""Repare na mistura entre dados numéricos e categóricos. Se o nosso modelo entende números, o que podemos fazer? Podemos transformar essas palavras em números. Para fazermos isso vamos usar um Label Enconder do sklearn."""

from sklearn.preprocessing import LabelEncoder

"""Assim como com o simple imputer, precisamos criar um label enconder e entrão fazer um fit transform nos dados."""

encoder = LabelEncoder()

"""E quais colunas são as categóricas? Vamos descobrir e passar essa lista para o nosso encoder."""

colunas_categoricas = [coluna for coluna in X_treino.columns if X_treino[coluna].dtype == "object"]

colunas_categoricas

X_treino_encodado = encoder.fit_transform(X_treino[colunas_categoricas])

"""Repare que ao fazermos isso deu um erro no nosso encoder de bad input shape, o que será que é isso?

Vamos dar uma olhada na coluna chamada Condition2 do nosso dataset.
"""

X_treino['Condition2']

"""Repare que temos diversas palavras aqui, e como o nosso encoder vai 'encodar' cada uma delas, ele vai criar uma representação para cada palavra diferente aqui, então vamos entender quantas palavras diferentes temos. Para isso usamos unique()."""

X_treino['Condition2'].unique()

"""Legal, certo, só que ainda não nos diz muito desse erro de diferença de tamanho. Repare que no erro tem alguma coisa de validation ali. E se olharmos para os valores únicos dessa mesma coluna do nosso X_valid também."""

X_valid['Condition2'].unique()

"""Repare que temos menos valores únicos em validação do que em treino. Por isso estamos tendo esse erro de tamanho de input.

Veja que estamos fazendo um fit nos nossos dados de treino e vamos transformar os nossos dados de validacao, só que isso não é possível, pois, por exemplo, eles tem tamanhos diferentes.

Como podemos nos adequar as caracteristicas de um dataset se elas são diferentes dependendo dos meus dados? Não podemos, então precisamos padronizar essas colunas.

Vamos escolher a abordagem mais simples e jogar fora colunas com tamanhos diferentes.

Para isso vamos usar algo chamado set. Lembra daquelas bolinhas de teoria de conjuntos no ensino fundamental de matemática? É isso mesmo.
"""

set(X_treino['Condition2']) == set(X_valid['Condition2'])

"""Primeiro vamos definir para quais colunas o encoding é válido."""

boas_colunas = [coluna for coluna in colunas_categoricas if 
                   set(X_treino[coluna]) == set(X_valid[coluna])]

"""Assim pegamos as boas colunas para o encoding, agora, como estamos lidando com conjuntos, se temos o conjunto de todas as colunas e pegamos as boas, a diferença disso deixa as ruins.

Vou coloca-las em uma lista.
"""

mas_colunas = list(set(colunas_categoricas)-set(boas_colunas))

print('Colunas categóricas que terão encoding:', boas_colunas)
print('Colunas categóricas que droparemos:', mas_colunas)

"""Beleza? Vamos realizar mais essa etapa de pre-processamento dos nossos dados então."""

X_treino_sem_mas_colunas = X_treino.drop(mas_colunas, axis=1)
X_valid_sem_mas_colunas = X_valid.drop(mas_colunas, axis=1)

"""Agora podemos continuar a aplicar o nosso encoder."""

for coluna in set(boas_colunas):
    X_treino_sem_mas_colunas[coluna] = encoder.fit_transform(X_treino[coluna])
    X_valid_sem_mas_colunas[coluna] = encoder.transform(X_valid[coluna])

"""E, pronto, nossas colunas estão encodadas. Agora é partir para modelo e calculo de erro.

Repare que ficou um pouco cansativo copiar e colar essas linhas, sempre fazemos o fit, o predict e calculamos o erro, então vamos separar isso em funções. De agora em diante, podemos apenas chamar a função e passar os parâmetros.
"""

def calcula_metrica_dataset(X_treino, X_valid, y_treino, y_valid):
    modelo = RandomForestRegressor(n_estimators=50, random_state=42)
    modelo.fit(X_treino, y_treino)
    preds = modelo.predict(X_valid)
    return mean_absolute_error(y_valid, preds)

calcula_metrica_dataset(X_treino_sem_mas_colunas, X_valid_sem_mas_colunas, y_treino, y_valid)

"""Inclusive podemos chamar a função com um print."""

print('MAE para o dataset com label encoding: %d' %(calcula_metrica_dataset(X_treino_sem_mas_colunas, X_valid_sem_mas_colunas, y_treino, y_valid)))

"""Repare que agora estamos usando variáveis categóricas e numéricas e o nosso erro foi menor do que usando apenas as numéricas ou retirando colunas demais do nosso dataset.

Será que ainda tem mais uma abordagem que podemos explorar para os nossos dados categóricos? Tem sim.

Quando vamos fazer uma conta, podemos trabalhar com números grandes, então somar 1000 com 1000 com 1000 ou podemos dividir esse 1000 por 100 e somar 10 com 10 com 10. Fazer essa segunda tarefa fica mais fácil do que a primeira tanto para a gente, quanto para a máquina.
"""

X_treino_sem_mas_colunas.head(5)

"""Olhando o nosso X treino com label encoding, temos números inteiros para representar as palavras e quanto mais valores diferentes tivermos, maiores eles serão.
Será que conseguimos reduzir esses números de alguma forma? Isso ajudaria o nosso modelo.

Se descrevermos o número 3 por exemplo, ao invés de 3 como 0 0 1, podemos fazer isso, mas agora repare que temos um array [0, 0, 1] que significa 3 ao invés de um único valor.

Essa é a próxima abordagem que vamos testar, mas ela só é indicada quando temos unique values baixos no nosso dataset, vamos entender primeiro como está isso.

Primeiro, vamos obter os valores únicos de cada coluna, para isso vamos usar n unique()
"""

# aqui estamos usando uma função anônima lambda que pega os valores unicos de cada coluna e aplicamos map para transformar isso em uma lista.
valores_nunicos = list(map(lambda coluna: X_treino[coluna].nunique(), colunas_categoricas))
# feito isso, criamos um dicionário com o nome da coluna e os valores
# colocamos os dois juntos com zip e transformamos em dicionario com dict
dicionario = dict(zip(colunas_categoricas, valores_nunicos))

"""Agora, vamos imprimir o número dessas entradas por coluna em ordem crescente."""

# como queremos que a ordem seja definida pelo número, que é a posição [1] de cada item do dict
# passamos uma função anônima que pega esse primeiro elemento para fazer isso
sorted(dicionario.items(), key=lambda x: x[1])

"""Cada um desses valores unicos por colunas de variáveis categóricas que estãmos vendo aqui são chamados de **cardinalidade**. 
Um bom parâmetro para nos basearmos para o nosso dataset não ficar gigante com os vetores de 0s e 1s é que caso a cardinalidade seja menor que 10, fazemos o encoding, senão, não e aí droparemos a coluna.

Repare que das colunas que temos, vamos dropar 3.

Vamos começar selecionando e guardando essas colunas.
"""

colunas_com_baixa_card = [coluna for coluna in colunas_categoricas if X_treino[coluna].nunique() < 10]

colunas_com_alta_card = list(set(colunas_categoricas)-set(colunas_com_baixa_card))

"""Vamos printar para conferir."""

print('Colunas categóricas que terão encoding 0 e 1:', colunas_com_baixa_card)
print('Colunas categóricas que droparemos:', colunas_com_alta_card)

"""Beleza, agora vamos começar. Esse encoding com 0 e 1 é chamado de **one hot encoding**. Vamos importar a lib do sklearn."""

from sklearn.preprocessing import OneHotEncoder

oh_encoder = OneHotEncoder()
colunas_treino_oh = pd.DataFrame(oh_encoder.fit_transform(X_treino[colunas_com_baixa_card]))
colunas_valid_oh = pd.DataFrame(oh_encoder.transform(X_valid[colunas_com_baixa_card]))

"""Repare que deu um erro de categorias desconhecidas, como é apenas numa categoria na coluna 12, vamos preenchê-la com 0s, a ignorando no dataset. Fazemos isso com handle_unknown='ignore'."""

oh_encoder = OneHotEncoder(handle_unknown='ignore')
colunas_treino_oh = pd.DataFrame(oh_encoder.fit_transform(X_treino[colunas_com_baixa_card]))
colunas_valid_oh = pd.DataFrame(oh_encoder.transform(X_valid[colunas_com_baixa_card]))

"""Vamos dar uma olhada nos nossos dados."""

colunas_treino_oh

"""Vish, o que aconteceu? Repare que se olharmos os valores default do one hot encoder, um deles é nos devolver uma matris esparsa, se queremos o array que tinhamos falado antes, precisamos alterar esse parâmtero. Vamos fazer isso adicionando sparse = False no encoder."""

oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
colunas_treino_oh = pd.DataFrame(oh_encoder.fit_transform(X_treino[colunas_com_baixa_card]))
colunas_valid_oh = pd.DataFrame(oh_encoder.transform(X_valid[colunas_com_baixa_card]))

colunas_treino_oh

"""Agora sim temos o resultado que queríamos. Legal, só que perdemos qualquer referência do índice, vamos adicionar essa referência novamente."""

colunas_treino_oh.index = X_treino.index
colunas_valid_oh.index = X_valid.index

colunas_treino_oh

"""Beleza, e agora? Até aqui nos pegamos as colunas categóricas com dimensionalidade < 10 do nosso dataset e fizemos o ine hot encoding nelas.E o resto do dataset, certo? E os dados numéricos?

O que precisamos fazer agora é retirar as colunas categóricas do nosso dataset, substitui-las pelo one hot encoding e então juntar isso com as colunas numéricas.
"""

X_treino_numerico = X_treino.drop(colunas_categoricas, axis=1)
X_valid_numerico = X_valid.drop(colunas_categoricas, axis=1)

"""Para juntar ambas, concatená-las, vamos usar o concat do pandas."""

X_treino_oh = pd.concat([X_treino_numerico, colunas_treino_oh], axis=1)
X_valid_oh = pd.concat([X_valid_numerico, colunas_valid_oh], axis=1)

"""Pronto, agora podemos chamar a nossa função e calcular o erro."""

print('MAE para o dataset com label encoding: %d' %(calcula_metrica_dataset(X_treino_sem_mas_colunas, X_valid_sem_mas_colunas, y_treino, y_valid)))
print('MAE para o dataset com one hot encoding: %d' %(calcula_metrica_dataset(X_treino_oh, X_valid_oh, y_treino, y_valid)))

"""Repare que usar one hot encoding nos deu um erro menor.

Vamos novamente fazer as predições e salvar o resultado. Antes, vamos alterar a nossa função para que ela nos devolva o modelo também.
"""

def calcula_metrica_dataset(X_treino, X_valid, y_treino, y_valid):
    modelo = RandomForestRegressor(n_estimators=50, random_state=42)
    modelo.fit(X_treino, y_treino)
    preds = modelo.predict(X_valid)
    return mean_absolute_error(y_valid, preds), modelo

mae, modelo = calcula_metrica_dataset(X_treino_oh, X_valid_oh, y_treino, y_valid)

"""E vamos agora fazer as predicoes para X teste."""

preds_teste = modelo.predict(X_teste)

"""Puxa, não encodamos X_teste, então vamos lá."""

X_teste_oh = pd.DataFrame(oh_encoder.transform(X_teste))

"""Putz, não tiramos os NaN de X teste."""

X_teste.drop(colunas_com_valores_faltantes, axis=1, inplace=True)

"""E agora deu outro erro por causa das diferenças de colunas.

Repare que vai ser trabalhoso repetirmos todo esse processo de one hot encoding para X_teste também, será que tem um jeito mais fácil?
"""

#resultados_oh = pd.DataFrame({'Id': X_teste.index, 'SalePrice': preds_teste})
#resultados_oh.to_csv('resultados_one_hot.csv', index=False)