# -*- coding: utf-8 -*-
"""canalizacoes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uHqykKI_N9kJQkqb19o6xfihhU_paWPz

Fazer todos esses pré processamentos para o conjunto de dados é desgastante. Há sim uma maneira de 'automatizar' isso, essa maneria constrói como que um canal por onde passam os nossos dados. O que chamamos de **pipeline**.

Antes de vermos o que é isso, vamos novamente fazer os imports, ler e limpar os nossos dados.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

dados_treino = pd.read_csv('treino.csv', index_col='Id')
dados_teste = pd.read_csv('teste.csv', index_col='Id')

"""Fazendo cópia, removendo linhas sem o valor que queremos prever e a coluna que vamos prever dos dados de treino"""

X = dados_treino.copy()

X.dropna(axis=0, subset=['SalePrice'], inplace=True)
y = X.SalePrice
X.drop(['SalePrice'], axis=1, inplace=True)

"""Feito essa limpeza, dividimos os datasets entre treino e teste."""

X_treino, X_valid, y_treino, y_valid = train_test_split(X, y, 
                                                        train_size=0.8, 
                                                        test_size=0.2, 
                                                        random_state=42)

"""Dos dados de treino, vamos selecionar as colunas categóricas com baixa cardinalidade, menor do que 10."""

colunas_cat = [coluna for coluna in X_treino.columns if
                    X_treino[coluna].nunique() < 10 and 
                    X_treino[coluna].dtype == "object"]

"""Agora vamos selecionar as colunas numéricas."""

colunas_num = [coluna for coluna in X_treino.columns if 
               X_treino[coluna].dtype in ['int64', 'float64']]

"""Das colunas que selecionamos, vamos manter apenas elas nos dados de validacao, treino e teste."""

colunas_selecionadas = colunas_cat + colunas_num

X_treino_selec = X_treino[colunas_selecionadas].copy()
X_valid_selec = X_valid[colunas_selecionadas].copy()
X_teste_selec = dados_teste[colunas_selecionadas].copy()

"""Beleza, agora podemos organizar a nossa pipeline.

Ela vai ser algo assim:
"""

pipeline_1 = Pipeline(steps = [(preprocessador,
                                modelo)])

"""Para isso, precisamos criar os preprocessadores. Quais são eles?

Os imputadores e encoders para processar os dados numericos e categoricos. Certo?

Como eles usam o fit transform, vamos chama-los de transformers.
"""

transformer_num = SimpleImputer(strategy='constant')

transformer_cat = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessador = ColumnTransformer(
    transformers=[
        ('transformer_num', transformer_num, colunas_num),
        ('transformer_cat', transformer_cat, colunas_cat)])

modelo = RandomForestRegressor(n_estimators=50, random_state=42)

pipeline_1 = Pipeline(steps=[('preprocessador', preprocessador),
                      ('modelo', modelo)])

pipeline_1.fit(X_treino_selec, y_treino)

preds = pipeline_1.predict(X_valid_selec)

print('MAE:', mean_absolute_error(y_valid, preds))

preds_teste = pipeline_1.predict(X_teste_selec)

resultado = pd.DataFrame({'Id': X_teste_selec.index,
                       'SalePrice': preds_teste})
resultado.to_csv('resultado_pipeline.csv', index=False)